{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset too small to split. Using the entire dataset for training and evaluation.\n",
      "Training model: xlm-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\SolomonProjects\\Kifiya\\week 5\\week-5\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.36s/ examples]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.24s/ examples]\n",
      "d:\\Projects\\SolomonProjects\\Kifiya\\week 5\\week-5\\venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:06<00:13,  6.50s/it]\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:07<00:13,  6.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1206722259521484, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9612676056338029, 'eval_runtime': 0.6924, 'eval_samples_per_second': 1.444, 'eval_steps_per_second': 1.444, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:12<00:06,  6.25s/it]\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:13<00:06,  6.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0189059972763062, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9647887323943662, 'eval_runtime': 1.2572, 'eval_samples_per_second': 0.795, 'eval_steps_per_second': 0.795, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:20<00:00,  6.89s/it]d:\\Projects\\SolomonProjects\\Kifiya\\week 5\\week-5\\venv\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Projects\\SolomonProjects\\Kifiya\\week 5\\week-5\\venv\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:29<00:00,  9.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.965729832649231, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9647887323943662, 'eval_runtime': 1.1206, 'eval_samples_per_second': 0.892, 'eval_steps_per_second': 0.892, 'epoch': 3.0}\n",
      "{'train_runtime': 29.4889, 'train_samples_per_second': 0.102, 'train_steps_per_second': 0.102, 'train_loss': 1.1376570065816243, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 90.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results for xlm-roberta-base: {'eval_loss': 0.965729832649231, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9647887323943662, 'eval_runtime': 1.0934, 'eval_samples_per_second': 0.915, 'eval_steps_per_second': 0.915, 'epoch': 3.0}\n",
      "Training model: distilroberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'DistilBertTokenizerFast'.\n",
      "d:\\Projects\\SolomonProjects\\Kifiya\\week 5\\week-5\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading model distilroberta-base: \n",
      " requires the protobuf library but it was not found in your environment. Checkout the instructions on the\n",
      "installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Training model: bert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.89s/ examples]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.64s/ examples]\n",
      "d:\\Projects\\SolomonProjects\\Kifiya\\week 5\\week-5\\venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:07<00:14,  7.41s/it]\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:08<00:14,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7742921710014343, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9090909090909091, 'eval_runtime': 0.6847, 'eval_samples_per_second': 1.46, 'eval_steps_per_second': 1.46, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:13<00:06,  6.57s/it]\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:14<00:06,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5040915012359619, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9602272727272727, 'eval_runtime': 0.92, 'eval_samples_per_second': 1.087, 'eval_steps_per_second': 1.087, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:18<00:00,  5.95s/it]d:\\Projects\\SolomonProjects\\Kifiya\\week 5\\week-5\\venv\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Projects\\SolomonProjects\\Kifiya\\week 5\\week-5\\venv\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:24<00:00,  8.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.42024707794189453, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9602272727272727, 'eval_runtime': 1.0443, 'eval_samples_per_second': 0.958, 'eval_steps_per_second': 0.958, 'epoch': 3.0}\n",
      "{'train_runtime': 24.3896, 'train_samples_per_second': 0.123, 'train_steps_per_second': 0.123, 'train_loss': 0.8668272495269775, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 55.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results for bert-base-multilingual-cased: {'eval_loss': 0.42024707794189453, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9602272727272727, 'eval_runtime': 1.6265, 'eval_samples_per_second': 0.615, 'eval_steps_per_second': 0.615, 'epoch': 3.0}\n",
      "\n",
      "Model Comparison Results:\n",
      "xlm-roberta-base: Precision=0.0000, Recall=0.0000, F1=0.0000, Accuracy=0.9648\n",
      "bert-base-multilingual-cased: Precision=0.0000, Recall=0.0000, F1=0.0000, Accuracy=0.9602\n",
      "Model and tokenizer for xlm-roberta-base saved successfully!\n",
      "Model and tokenizer for distilroberta-base saved successfully!\n",
      "Model and tokenizer for bert-base-multilingual-cased saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (XLMRobertaTokenizerFast, XLMRobertaForTokenClassification, \n",
    "                          DistilBertTokenizerFast, DistilBertForTokenClassification,\n",
    "                          BertTokenizerFast, BertForTokenClassification,\n",
    "                          Trainer, TrainingArguments, DataCollatorForTokenClassification)\n",
    "import evaluate\n",
    "\n",
    "# Load the labeled dataset in CoNLL format\n",
    "def load_conll_data(file_path):\n",
    "    \"\"\"Loads CoNLL formatted data into a pandas DataFrame.\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        sentence = []\n",
    "        labels = []\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                token, label = line.strip().split()\n",
    "                sentence.append(token)\n",
    "                labels.append(label)\n",
    "            else:\n",
    "                data.append((sentence, labels))\n",
    "                sentence = []\n",
    "                labels = []\n",
    "    if sentence:  # For the last sentence if there is no newline\n",
    "        data.append((sentence, labels))\n",
    "    return data\n",
    "\n",
    "conll_file_path = '../output/labeled_telegram_data.conll'\n",
    "data = load_conll_data(conll_file_path)\n",
    "\n",
    "# Convert data into a DataFrame\n",
    "df = pd.DataFrame(data, columns=['tokens', 'labels'])\n",
    "\n",
    "# Check the size of the dataset\n",
    "if len(df) > 1:\n",
    "    # Split into train and validation sets if you have more than one sample\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2)\n",
    "else:\n",
    "    # If the dataset is too small, use the entire dataset for both training and validation\n",
    "    print(\"Dataset too small to split. Using the entire dataset for training and evaluation.\")\n",
    "    train_df = df\n",
    "    val_df = df\n",
    "\n",
    "# Convert to Hugging Face dataset format\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "# Tokenization and alignment of labels (same function as before)\n",
    "def tokenize_and_align_labels(examples, tokenizer):\n",
    "    \"\"\"Tokenizes inputs and aligns labels.\"\"\"\n",
    "    tokenized_inputs = tokenizer(examples['tokens'], is_split_into_words=True, padding=True, truncation=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['labels']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # get word ids\n",
    "        label_ids = [-100] * len(tokenized_inputs['input_ids'][i])  # default to -100 (ignore index)\n",
    "\n",
    "        # Align labels with tokenized inputs\n",
    "        for j, label_id in enumerate(label):\n",
    "            if j < len(word_ids) and word_ids[j] is not None:  # avoid IndexError\n",
    "                if label_id in label_map:  # Check if label_id exists in label_map\n",
    "                    label_ids[word_ids[j]] = label_map[label_id]  # map label to its id\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs['labels'] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Map label names to IDs\n",
    "label_list = list(set(label for labels in df['labels'] for label in labels))\n",
    "label_list = sorted(label_list)\n",
    "label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "# Metrics function using Hugging Face's `evaluate` library\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "\n",
    "    # Remove ignored index (-100)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "# Define model names for comparison\n",
    "model_names = [\n",
    "    \"xlm-roberta-base\",  # XLM-Roberta\n",
    "    \"distilroberta-base\",  # DistilRoBERTa\n",
    "    \"bert-base-multilingual-cased\",  # mBERT\n",
    "]\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"Training model: {model_name}\")\n",
    "    \n",
    "    # Load tokenizer and model based on the model name\n",
    "    try:\n",
    "        if model_name == \"xlm-roberta-base\":\n",
    "            tokenizer = XLMRobertaTokenizerFast.from_pretrained(model_name)\n",
    "            model = XLMRobertaForTokenClassification.from_pretrained(model_name, num_labels=len(label_list))\n",
    "        elif model_name == \"distilroberta-base\":\n",
    "            tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "            model = DistilBertForTokenClassification.from_pretrained(model_name, num_labels=len(label_list))\n",
    "        elif model_name == \"bert-base-multilingual-cased\":\n",
    "            tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "            model = BertForTokenClassification.from_pretrained(model_name, num_labels=len(label_list))\n",
    "\n",
    "        # Initialize the data collator (moved inside the loop)\n",
    "        data_collator = DataCollatorForTokenClassification(tokenizer, padding=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model {model_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Tokenize the datasets\n",
    "    tokenized_train_dataset = train_dataset.map(lambda x: tokenize_and_align_labels(x, tokenizer), \n",
    "                                                batched=True, remove_columns=['tokens', 'labels'])\n",
    "    tokenized_val_dataset = val_dataset.map(lambda x: tokenize_and_align_labels(x, tokenizer), \n",
    "                                             batched=True, remove_columns=['tokens', 'labels'])\n",
    "\n",
    "    # Set up training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'../results/{model_name}',  # output directory\n",
    "        evaluation_strategy=\"epoch\",           # evaluate every epoch\n",
    "        learning_rate=2e-5,                    # learning rate\n",
    "        per_device_train_batch_size=16,        # batch size for training\n",
    "        per_device_eval_batch_size=16,         # batch size for evaluation\n",
    "        num_train_epochs=3,                    # total number of training epochs\n",
    "        weight_decay=0.01,                     # strength of weight decay\n",
    "        logging_dir=f'../logs/{model_name}',   # directory for storing logs\n",
    "    )\n",
    "\n",
    "    # Create a Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_val_dataset,   # Use validation set for evaluation\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the fine-tuned model on the validation set\n",
    "    eval_results = trainer.evaluate()\n",
    "    results[model_name] = eval_results\n",
    "    print(f\"Evaluation results for {model_name}: {eval_results}\")\n",
    "\n",
    "# Print summary of results\n",
    "print(\"\\nModel Comparison Results:\")\n",
    "for model_name, result in results.items():\n",
    "    print(f\"{model_name}: Precision={result['eval_precision']:.4f}, Recall={result['eval_recall']:.4f}, F1={result['eval_f1']:.4f}, Accuracy={result['eval_accuracy']:.4f}\")\n",
    "\n",
    "# Step 7: Save the fine-tuned models\n",
    "for model_name in model_names:\n",
    "    model.save_pretrained(f'../models/{model_name}')\n",
    "    tokenizer.save_pretrained(f'../models/{model_name}')\n",
    "    print(f\"Model and tokenizer for {model_name} saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
